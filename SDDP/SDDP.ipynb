{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-first-steps\" data-toc-modified-id=\"1.-first-steps-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>1. first steps</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-Mathematical-formulation\" data-toc-modified-id=\"1.1-Mathematical-formulation-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>1.1 Mathematical formulation</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1.1-Stages\" data-toc-modified-id=\"1.1.1-Stages-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>1.1.1 Stages</a></span></li><li><span><a href=\"#1.1.2-State-variables\" data-toc-modified-id=\"1.1.2-State-variables-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>1.1.2 State variables</a></span></li><li><span><a href=\"#1.1.3-Control-variables\" data-toc-modified-id=\"1.1.3-Control-variables-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>1.1.3 Control variables</a></span></li><li><span><a href=\"#1.1.4-The-dynamics\" data-toc-modified-id=\"1.1.4-The-dynamics-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>1.1.4 The dynamics</a></span></li><li><span><a href=\"#1.1.5-The-stage-objective\" data-toc-modified-id=\"1.1.5-The-stage-objective-1.1.5\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;</span>1.1.5 The stage-objective</a></span></li></ul></li><li><span><a href=\"#1.2.-Creating-a-model\" data-toc-modified-id=\"1.2.-Creating-a-model-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>1.2. Creating a model</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.2.1-A-simple-model\" data-toc-modified-id=\"1.2.1-A-simple-model-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>1.2.1 A simple model</a></span></li><li><span><a href=\"#1.2.2-The-do-syntax\" data-toc-modified-id=\"1.2.2-The-do-syntax-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>1.2.2 The <code>do</code> syntax</a></span></li></ul></li><li><span><a href=\"#1.3-A-first-hydro-model\" data-toc-modified-id=\"1.3-A-first-hydro-model-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>1.3 A first hydro model</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.3.1-The-keywords-in-the-SDDP.LinearPolicyGraph-constructor\" data-toc-modified-id=\"1.3.1-The-keywords-in-the-SDDP.LinearPolicyGraph-constructor-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>1.3.1 The keywords in the <code>SDDP.LinearPolicyGraph</code> constructor</a></span></li><li><span><a href=\"#1.3.2-Creating-state-variables\" data-toc-modified-id=\"1.3.2-Creating-state-variables-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>1.3.2 Creating state variables</a></span></li><li><span><a href=\"#1.3.3-Defining-the-stage-objective\" data-toc-modified-id=\"1.3.3-Defining-the-stage-objective-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>1.3.3 Defining the stage-objective</a></span></li></ul></li><li><span><a href=\"#1.4-Manipulating-policies\" data-toc-modified-id=\"1.4-Manipulating-policies-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>1.4 Manipulating policies</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.4.1-Training-a-policy\" data-toc-modified-id=\"1.4.1-Training-a-policy-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>1.4.1 Training a policy</a></span></li><li><span><a href=\"#1.4.2-Saving-the-policy\" data-toc-modified-id=\"1.4.2-Saving-the-policy-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>1.4.2 Saving the policy</a></span></li><li><span><a href=\"#1.4.3-Simulating-the-policy\" data-toc-modified-id=\"1.4.3-Simulating-the-policy-1.4.3\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span>1.4.3 Simulating the policy</a></span></li></ul></li><li><span><a href=\"#1.5-Exercise-:-a-hot-water-tank-problem\" data-toc-modified-id=\"1.5-Exercise-:-a-hot-water-tank-problem-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>1.5 Exercise : a hot water tank problem</a></span><ul class=\"toc-item\"><li><span><a href=\"#Question-1\" data-toc-modified-id=\"Question-1-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Question 1</a></span></li><li><span><a href=\"#Question-2\" data-toc-modified-id=\"Question-2-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Question 2</a></span></li></ul></li><li><span><a href=\"#Question-3-(Optional)\" data-toc-modified-id=\"Question-3-(Optional)-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Question 3 (Optional)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Question-3.a)\" data-toc-modified-id=\"Question-3.a)-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Question 3.a)</a></span></li><li><span><a href=\"#Question-3.b)\" data-toc-modified-id=\"Question-3.b)-1.6.2\"><span class=\"toc-item-num\">1.6.2&nbsp;&nbsp;</span>Question 3.b)</a></span></li></ul></li></ul></li><li><span><a href=\"#2.-adding-uncertainty\" data-toc-modified-id=\"2.-adding-uncertainty-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>2. adding uncertainty</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1-Creating-a-model\" data-toc-modified-id=\"2.1-Creating-a-model-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>2.1 Creating a model</a></span></li><li><span><a href=\"#2.2-Training-and-simulating-the-policy\" data-toc-modified-id=\"2.2-Training-and-simulating-the-policy-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>2.2 Training and simulating the policy</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.2.1-Training-the-policy\" data-toc-modified-id=\"2.2.1-Training-the-policy-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>2.2.1 Training the policy</a></span></li><li><span><a href=\"#2.2.2-Simulating-the-policy\" data-toc-modified-id=\"2.2.2-Simulating-the-policy-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>2.2.2 Simulating the policy</a></span></li><li><span><a href=\"#2.2.3-Simulating-further-parameters-(Optional)\" data-toc-modified-id=\"2.2.3-Simulating-further-parameters-(Optional)-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>2.2.3 Simulating further parameters (Optional)</a></span></li></ul></li><li><span><a href=\"#2.3-Exercise-:-Hot-water-with-random-demand\" data-toc-modified-id=\"2.3-Exercise-:-Hot-water-with-random-demand-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>2.3 Exercise : Hot water with random demand</a></span><ul class=\"toc-item\"><li><span><a href=\"#Question-4.\" data-toc-modified-id=\"Question-4.-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Question 4.</a></span></li></ul></li></ul></li><li><span><a href=\"#3.-Two-uncertainties\" data-toc-modified-id=\"3.-Two-uncertainties-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3. Two uncertainties</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1-Creating-a-model\" data-toc-modified-id=\"3.1-Creating-a-model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>3.1 Creating a model</a></span></li><li><span><a href=\"#3.2-Training-and-simulating-the-policy\" data-toc-modified-id=\"3.2-Training-and-simulating-the-policy-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>3.2 Training and simulating the policy</a></span></li></ul></li><li><span><a href=\"#4.-plotting\" data-toc-modified-id=\"4.-plotting-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>4. plotting</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1-Preliminaries\" data-toc-modified-id=\"4.1-Preliminaries-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>4.1 Preliminaries</a></span></li><li><span><a href=\"#4.2-Spaghetti-plots-(Optional)\" data-toc-modified-id=\"4.2-Spaghetti-plots-(Optional)-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>4.2 Spaghetti plots (Optional)</a></span></li><li><span><a href=\"#4.3-Publication-plots\" data-toc-modified-id=\"4.3-Publication-plots-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>4.3 Publication plots</a></span></li><li><span><a href=\"#4.4-Exercise\" data-toc-modified-id=\"4.4-Exercise-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>4.4 Exercise</a></span></li></ul></li><li><span><a href=\"#5-Exercise-:-ramp-up-constraints-(Optional)\" data-toc-modified-id=\"5-Exercise-:-ramp-up-constraints-(Optional)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>5 Exercise : ramp-up constraints (Optional)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eacd1be0a3a4628f63c5e385baa30918",
     "grade": false,
     "grade_id": "cell-78d0a201b3f73d07",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# A tutorial on SDDP\n",
    "\n",
    "By V. Lecl√®re largely based upon O. Dowson work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31681820cba25c0b191fe652ac43d020",
     "grade": false,
     "grade_id": "cell-ea033146550475b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "] add SDDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SDDP, GLPK, Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac1dda4c5c4c2352a0e9943239705bc4",
     "grade": false,
     "grade_id": "cell-685212e2f72dc7de",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1. first steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "805384ffaa0f39e10bd9810da3e60f04",
     "grade": false,
     "grade_id": "cell-5c0f4bd004c45840",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Hydrothermal scheduling is the most common application of stochastic dual\n",
    "dynamic programming. To illustrate some of the basic functionality of\n",
    "`SDDP.jl`, we implement a very simple model of the hydrothermal scheduling\n",
    "problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0fd743a70d7d32b5d867f1e0ba08203e",
     "grade": false,
     "grade_id": "cell-59d094b5bf75adc5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We consider the problem of scheduling electrical generation over three time\n",
    "periods in order to meet a known demand of 150 MWh in each period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a51c21e8d9f37211c1e831772dae86a",
     "grade": false,
     "grade_id": "cell-589db3f813e3fc5c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "There are two generators: a thermal generator, and a hydro generator. The\n",
    "thermal generator has a short-run marginal cost of \\\\\\$50/MWh in the first\n",
    "stage, \\\\\\$100/MWh in the second stage, and \\\\\\$150/MWh in the third stage.\n",
    "The hydro generator has a short-run marginal cost of \\\\\\$0/MWh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1ebb5a47f8ac3d3770cf44149b6a5b4",
     "grade": false,
     "grade_id": "cell-60d5d65b4d3d32a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The hydro generator draws water from a reservoir which has a maximum capacity\n",
    "of 200 units. We assume that at the start of the first time period, the\n",
    "reservoir is full. In addition to the ability to generate electricity by\n",
    "passing water through the hydroelectric turbine, the hydro generator can also\n",
    "spill water down a spillway (bypassing the turbine) in order to prevent the\n",
    "water from over-topping the dam. We assume that there is no cost of spillage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7f27840e178554f4ae9d127f30ce934",
     "grade": false,
     "grade_id": "cell-4c190f0495000ffa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The objective of the optimization is to minimize the expected cost of\n",
    "generation over the three time periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d882d34944f1f20be91dd9183d849a80",
     "grade": false,
     "grade_id": "cell-67f4a960be9f4bc1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1 Mathematical formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the problem described above and form a mathematical model. In any\n",
    "multistage stochastic programming problem, we need to identify five key\n",
    "features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The _stages_\n",
    "2. The _state_ variables\n",
    "3. The _control_ variables\n",
    "4. The _dynamics_\n",
    "5. The _stage-objective_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the description, we have three stages: `t = 1, 2, 3`. Here is a picture\n",
    "of what this looks like:\n",
    "\n",
    "![Linear policy graph](assets/deterministic_linear_policy_graph.png)\n",
    "\n",
    "Notice that the boxes form a _linear graph_. This will be important when we\n",
    "get to the code. (We'll get to more complicated graphs in future tutorials.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 State variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State variables capture the information that flows between stages. These can\n",
    "be harder to identify. However, in our model, the state variable is the volume\n",
    "of water stored in the reservoir over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model below, we're going to call the state variable `volume`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each stage `t` is an interval in time. Thus, we need to record the value of\n",
    "the state variable in each stage at two points in time: at the beginning of\n",
    "the stage, which we  refer to as the _incoming_ value of the state variable;\n",
    "and at the end of the  state, which we refer to as the _outgoing_ state\n",
    "variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to refer to the incoming value of `volume` by `volume.in` and the\n",
    "outgoing value by `volume.out`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `volume.out` when `t=1` is equal to `volume.in` when `t=2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem description also mentions some constraints on the volume of water\n",
    "in the reservoir. It cannot be negative, and the maximum level is 200 units.\n",
    "Thus, we have `0 <= volume <= 200`. Also, the description says that the\n",
    "initial value of water in the reservoir (i.e., `volume.in` when `t = 1`) is\n",
    "200 units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Control variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Control variables are the actions that the agent can take during a stage to\n",
    "change the value of the state variables. (Hence the name _control_.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three control variables in our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The quantity of thermal generation, which we're going to call\n",
    "   `thermal_generation`.\n",
    "2. The quantity of hydro generation, which we're going to call\n",
    "   `hydro_generation`.\n",
    "3. The quatity of water to spill, which we're going to call `hydro_spill`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these variables are non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 The dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamics of a problem describe how the state variables evolve through time\n",
    "in response to the controls chosen by the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our problem, the state variable is the volume of water in the reservoir.\n",
    "The volume of water decreases in response to water being used for hydro\n",
    "generation and spillage. So the dynamics for our problem are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`volume.out = volume.in - hydro_generation - hydro_spill`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also put constraints on the values of the state and control variables.\n",
    "For example, in our problem, there is also a constraint that the total\n",
    "generation must meet the demand of 150 MWh in each stage. So, we have a\n",
    "constraint that:\n",
    "`hydro_generation + thermal_generation = 150`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 1.1.5 The stage-objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent's objective is to minimize the cost of generation. So in each stage,\n",
    "the agent wants to minimize the quantity of thermal generation multiplied by\n",
    "the short-run marginal cost of thermal generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In stage `t`, they want to minimize `fuel_cost[t] * thermal_generation`, where\n",
    "`fuel_cost[t]` is \\\\\\$50 when `t=1`, \\\\\\$100 when `t=2`, and \\\\\\$150 when\n",
    "`t=3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to construct a model. Since `SDDP.jl` is intended to be very\n",
    "user-friendly, we're going to give the full code first, and then walk through\n",
    "some of the details. However, you should be able to read through and\n",
    "understand most of what is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Creating a model\n",
    "\n",
    "#### 1.2.1 A simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic element of `SDDP.jl` is a `LinearPolicyGraph` which represent the stochastic dynamic programm.\n",
    "\n",
    "In order to implement one, you first need a subproblem builder, which is a function defining a `JuMP` model incorporating stage costs and dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "c = [50.0, 100.0, 150.0]\n",
    "\n",
    "function subproblem_builder!(subproblem::JuMP.Model, t::Int)\n",
    "    # Define the state variable(s)\n",
    "    @variable(subproblem, 0 <= x <= 1, SDDP.State, initial_value = 0)\n",
    "    # Define the control variable(s)\n",
    "    @variable(subproblem, 0<= u <= 0.1)\n",
    "    # Define the constraint(s)\n",
    "    @constraint(subproblem,x.out == x.in + u)\n",
    "    # Define the objective for each stage `t`. \n",
    "    # Note that we can use `t` as an index for t = 1, 2, 3.\n",
    "    @stageobjective(subproblem, c[t] * u)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now construct our multistage model with `SDDP.LinearPolicyGraph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SDDP.LinearPolicyGraph(\n",
    "    subproblem_builder!, # a instantiating the JuMP model\n",
    "    stages = 3, # number of stages of the problem\n",
    "    sense = :Min, # minimization / maximization\n",
    "    lower_bound = 0.0, # a required lower bound on the global cost\n",
    "    optimizer = with_optimizer(GLPK.Optimizer) # the LP optimizer called\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 The `do` syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia's `do` syntax looks a little weird at first, but it's just a nice way of\n",
    "making a function that can be passed to another function. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function outer(inner::Function)\n",
    "    inner(2)\n",
    "end\n",
    "\n",
    "outer() do x\n",
    "    println(\"x^2 = \", x^2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is equivalent to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner(x) = println(\"x^2 = \", x^2)\n",
    "outer(inner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in our case, we could have gone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SDDP.LinearPolicyGraph(\n",
    "    stages = 3, # number of stages of the problem\n",
    "    sense = :Min, # minimization / maximization\n",
    "    lower_bound = 0.0, # a required lower bound on the global cost\n",
    "    optimizer = with_optimizer(GLPK.Optimizer) # the LP optimizer called\n",
    ") do subproblem, t\n",
    "@variable(subproblem, 0 <= x <= 1, SDDP.State, initial_value = 0)\n",
    "    # Define the control variable(s)\n",
    "    @variable(subproblem, 0<= u <= 0.1)\n",
    "    # Define the constraint(s)\n",
    "    @constraint(subproblem,x.out == x.in + u)\n",
    "    # Define the objective for each stage `t`. \n",
    "    # Note that we can use `t` as an index for t = 1, 2, 3.\n",
    "    @stageobjective(subproblem, c[t] * u)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 A first hydro model\n",
    "\n",
    "The previous problem was extremely simple, we are now turning toward a first hydro model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_model1 = SDDP.LinearPolicyGraph(\n",
    "    stages = 3,\n",
    "    sense = :Min,\n",
    "    lower_bound = 0.0,\n",
    "    optimizer = with_optimizer(GLPK.Optimizer)\n",
    ") do subproblem, t\n",
    "    # Define the state variable.\n",
    "    @variable(subproblem, 0 <= volume <= 200, SDDP.State, initial_value = 200)\n",
    "    # Define the control variables.\n",
    "    @variables(subproblem, begin #equivalent to multiple @variable lines\n",
    "        thermal_generation >= 0\n",
    "        hydro_generation   >= 0\n",
    "        hydro_spill        >= 0\n",
    "    end)\n",
    "    # Define the constraints\n",
    "    @constraints(subproblem, begin #equivalent to multiple @constraint lines\n",
    "        volume.out == volume.in - hydro_generation - hydro_spill\n",
    "        thermal_generation + hydro_generation == 150.0\n",
    "    end)\n",
    "    # Define the objective for each stage `t`. Note that we can use `t` as an\n",
    "    # index for t = 1, 2, 3.\n",
    "    fuel_cost = [50.0, 100.0, 150.0]\n",
    "    @stageobjective(subproblem, fuel_cost[t] * thermal_generation)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 The keywords in the `SDDP.LinearPolicyGraph` constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully `stages` and `sense` are obvious. However, the other two are not so\n",
    "clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lower_bound`: you _must_ supply a valid bound on the objective. For our\n",
    "problem, we know that we cannot incur a negative cost so \\\\\\$0 is a valid lower\n",
    "bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`optimizer`: This is borrowed directly from JuMP's `Model` constructor:\n",
    "(`Model(with_optimizer(GLPK.Optimizer))`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on `SDDP.LinearPolicyGraph`s, read\n",
    "    [Create a general policy graph](https://odow.github.io/SDDP.jl/latest/guides/create_a_general_policy_graph/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Creating state variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State variables can be created like any other JuMP variables. Think of them as\n",
    "another type of variable like binary or integer. For example, to create a\n",
    "binary variable in JuMP, you go:\n",
    "```julia\n",
    "@variable(subproblem, x, Bin)\n",
    "```\n",
    "whereas to create a state variable you go\n",
    "```julia\n",
    "@variable(subproblem, x, SDDP.State)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that you have to pass a keyword argument called `initial_value` that\n",
    "gives the incoming value of the state variable in the first stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Defining the stage-objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a JuMP model, we can set the objective using `@objective`. For example:\n",
    "```julia\n",
    "@objective(subproblem, Min, fuel_cost[t] * thermal_generation)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only need to define the objective for each stage, rather than the\n",
    "whole problem, we use the `SDDP.jl`-provided `@stageobjective`.\n",
    "```julia\n",
    "@stageobjective(subproblem, fuel_cost[t] * thermal_generation)\n",
    "```\n",
    "Note that we don't have to specify the optimization sense (`Max` of `Min`)\n",
    "since this is done via the `sense` keyword argument of\n",
    "`SDDP.LinearPolicyGraph`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1.4 Manipulating policies \n",
    " \n",
    " #### 1.4.1 Training a policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be trained using the `SDDP.train` function. It accepts a\n",
    "number of keyword arguments. `iteration_limit` terminates the training after\n",
    "the provided number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDDP.train(hydro_model1; iteration_limit = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Saving the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have finished training the policy, you can write the cuts to file\n",
    "using `SDDP.write_cuts_to_file`. You can read these cuts into a new\n",
    "model using `SDDP.read_cuts_from_file`. Note that the model must have\n",
    "the same number (and names) of the state variables, as well as the same number\n",
    "and names of the nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also save the log to a CSV file using `SDDP.write_log_to_csv`.\n",
    "This will create a CSV file with columns `iteration`, `simulation`, `bound`,\n",
    "and `time`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 Simulating the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a trained policy, you can simulate it using\n",
    "`SDDP.simulate`. The return value from `simulate` is a vector with one\n",
    "element for each replication. Each element is itself a vector, with one\n",
    "element for each stage. Each element, corresponding to a particular stage in a\n",
    "particular replication, is a dictionary that records information from the\n",
    "simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations = SDDP.simulate(\n",
    "    # The trained model to simulate.\n",
    "    hydro_model1,\n",
    "    # The number of replications.\n",
    "    1,\n",
    "    # A list of names to record the values of.\n",
    "    [:volume, :thermal_generation, :hydro_generation, :hydro_spill]\n",
    ")\n",
    "\n",
    "replication = 1\n",
    "stage = 1\n",
    "\n",
    "simulations[replication][stage]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore many of the entries for now;  they will be relevant later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " One element of interest is `:volume`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outgoing_volume = [stage[:volume].out for stage in simulations[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Another is `:thermal_generation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thermal_generation = [stage[:thermal_generation] for stage in simulations[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can see the optimal policy: in the first stage, use 150 MWh of\n",
    "thermal generation and 0 MWh of hydro generation. In the second stage, use 100\n",
    "MWh of thermal and 50 MWh of hydro. In the third and final stage, use 0 MWh of\n",
    "thermal and 150 MWh of  hydro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "008e6ca535b067ffaa2ac3ba4f4ef076",
     "grade": false,
     "grade_id": "cell-571e8c8cc0a74c48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.5 Exercise : a hot water tank problem\n",
    "\n",
    "We consider a hot water tank for a residential building. We model the water tank as an equivalent amount of hot water at each hour $ 0 \\leq s_t \\leq 500$. We assume that it follows the dynamic\n",
    "$$ s_0 = 200, \\qquad s_{t+1} = \\rho s_t + u_t - d_t $$\n",
    "where $u_t \\leq 200$ is the amount of warmed water between $t$ and $t+1$, for a cost $c_t$ per unit, and $d_t$ the amount of extracted water. The coefficient $0<\\rho <1$ represent the heat loss.\n",
    "\n",
    "We assume that we have the following data ($t=1$ corresponds to 4am) \n",
    "\n",
    "| t     ||  1    |  2   | 3    | 4    |  5   | 6    | 7    |  8   | 9    |\n",
    "| ----  || ---   | ---  | ---  | ---  | ---  | ---  | ---  | ---  | ---  |\n",
    "| $c_t$ ||  30   | 28.5 | 42.5 | 50.4 |¬†52.6 | 51.8 |¬†48.9 | 48.6 | 47.04|\n",
    "| $d_t$ ||  0    | 20   | 200  | 150  |¬†50   | 20   |¬†20   | 30   | 50   |\n",
    "\n",
    "We want to define an optimal heating strategy. Hot water after $t=9$ is considered lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66b96a44811f4fb147f7b7ad4ae74af1",
     "grade": false,
     "grade_id": "cell-6bafa282de24c035",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "question"
    ]
   },
   "source": [
    "#### ¬†Question 1 \n",
    "\n",
    "Construct a `hw_model` representing this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d08a5a8285bb1eb910ec34f37b538d6d",
     "grade": true,
     "grade_id": "cell-88153b9d54d0236e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "œÅ = 0.9\n",
    "c = [30 28.5 42.5 50.4 52.6 51.8 48.9 48.6 47.04]\n",
    "d = [0    20   200  150  50   20   20   30   50]\n",
    "\n",
    "\n",
    "hw_model1 = SDDP.LinearPolicyGraph(\n",
    "    \n",
    "    stages = 9, \n",
    "    sense = :Min,\n",
    "    lower_bound = 0.0, \n",
    "    optimizer = with_optimizer(GLPK.Optimizer)\n",
    ") do subproblem, t\n",
    "    #YOUR CODE HERE\n",
    "    error(\"to be implemented\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69b42694e6d4f95f20d4fc005e32ef2a",
     "grade": false,
     "grade_id": "cell-c16ff7e8b0ea0339",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "question"
    ]
   },
   "source": [
    "#### Question 2\n",
    "\n",
    "Train and simulate the problem. Describe the optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7b7995429f92cae252e3afdea3270fb",
     "grade": true,
     "grade_id": "cell-79edccf3a8c47007",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "error(\"to be implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd532fa35a48f5cc8dcf14758ce944d0",
     "grade": false,
     "grade_id": "cell-a4d1b23c8f2e25a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "question"
    ]
   },
   "source": [
    "### Question 3 (Optional)\n",
    "\n",
    "We are now going to consider a more contrained version of our problem, where $u \\leq 100$. \n",
    "\n",
    "#### Question 3.a)\n",
    "Is this new problem admissible ? Are we in a relatively complete recourse framework ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73833fc98c4e51d14e8c18e26faf7e90",
     "grade": true,
     "grade_id": "cell-ffed484903784677",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2635e12ad02e1c60e2aa969f82df04e1",
     "grade": false,
     "grade_id": "cell-4a9204f68ced31b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Question 3.b)\n",
    "Solving this new problem via SDDP won't work (try it !). \n",
    "Understand the difficulty and by introducing a new variable solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09fd86d2613a749fe850a254c3c96a0b",
     "grade": true,
     "grade_id": "cell-7643869a5f319170",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "hw_model2 = SDDP.LinearPolicyGraph(\n",
    "    stages = 9, \n",
    "    sense = :Min,\n",
    "    lower_bound = 0.0, \n",
    "    optimizer = with_optimizer(GLPK.Optimizer)\n",
    ") do subproblem, t\n",
    "    #YOUR CODE HERE\n",
    "    error(\"to be implemented\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0908b30c675c8be05c583e4078e69ba1",
     "grade": true,
     "grade_id": "cell-ae4d13702ee6a67c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Train and simulate\n",
    "\n",
    "#YOUR CODE HERE\n",
    "error(\"to be implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. adding uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now we have solved a deterministic  hydro-thermal scheduling model. We are now adding uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably missing from our previous model were inflows. Inflows are the water\n",
    "that flows into the reservoir through rainfall or rivers. These inflows are\n",
    "uncertain, and are the cause of the main trade-off in hydro-thermal\n",
    "scheduling: the desire to use water now to generate cheap electricity, against\n",
    "the risk that future inflows will be low, leading to blackouts or expensive\n",
    "thermal generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our simple hydro model, we assume that the inflows can be modelled by a discrete\n",
    "distribution with the three outcomes given in the following table:\n",
    "\n",
    "| œâ    |   0 |  50 | 100 |\n",
    "| ---- | --- | --- | --- |\n",
    "| P(œâ) | 1/3 | 1/3 | 1/3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the noise (the random variable) is observed by the agent at the\n",
    "start of each stage. This makes the problem a _wait-and-see_ or\n",
    "_hazard-decision_ formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent this, we can draw the following picture. The wavy lines denote\n",
    "the uncertainty arriving into the start of each stage (node).\n",
    "\n",
    "![Linear policy graph](assets/stochastic_linear_policy_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to adding this uncertainty to the model, we also need to modify\n",
    "the dynamics to include `inflow`:\n",
    "\n",
    "`volume.out = volume.in + inflow - hydro_generation - hydro_spill`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add an uncertain variable to the model, we create a new JuMP variable\n",
    "`inflow`, and then call the function `SDDP.parameterize`. The\n",
    "`SDDP.parameterize` function takes three arguments: the subproblem, a\n",
    "vector of realizations, and a corresponding vector of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_model2 = SDDP.LinearPolicyGraph(\n",
    "    stages = 3,\n",
    "    sense = :Min,\n",
    "    lower_bound = 0.0,\n",
    "    optimizer = with_optimizer(GLPK.Optimizer)\n",
    ") do subproblem, t\n",
    "    # Define the state variable.\n",
    "    @variable(subproblem, 0 <= volume <= 200, SDDP.State, initial_value = 200)\n",
    "    # Define the control variables.\n",
    "    @variables(subproblem, begin\n",
    "        thermal_generation >= 0\n",
    "        hydro_generation   >= 0\n",
    "        hydro_spill        >= 0\n",
    "        inflow\n",
    "    end)\n",
    "    # Define the constraints\n",
    "    @constraints(subproblem, begin\n",
    "        volume.out == volume.in + inflow - hydro_generation - hydro_spill\n",
    "        demand_constraint, thermal_generation + hydro_generation == 150.0\n",
    "    end)\n",
    "    # Define the objective for each stage `t`. Note that we can use `t` as an\n",
    "    # index for t = 1, 2, 3.\n",
    "    fuel_cost = [50.0, 100.0, 150.0]\n",
    "    @stageobjective(subproblem, fuel_cost[t] * thermal_generation)\n",
    "    # Parameterize the subproblem.\n",
    "    SDDP.parameterize(subproblem, [0.0, 50.0, 100.0], [1/3, 1/3, 1/3]) do œâ\n",
    "        JuMP.fix(inflow, œâ)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we use the JuMP function\n",
    "[`JuMP.fix`](http://www.juliaopt.org/JuMP.jl/v0.19/variables/#JuMP.fix) to set\n",
    "the value of the `inflow` variable to `œâ`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Warning :</b> `SDDP.parameterize` can only be called once in each subproblem definition!\n",
    "</div>\n",
    "\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training and simulating the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Training the policy\n",
    "\n",
    "As earlier we train the policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDDP.train(hydro_model2; iteration_limit = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Simulating the policy\n",
    "\n",
    "We can also simulate the policy. Note that this time, the simulation is\n",
    "stochastic. One common approach to quantify the quality of the policy is to\n",
    "perform  a Monte Carlo simulation and then form a confidence interval for the\n",
    "expected cost. This confidence interval is an estimate for the upper bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the confidence interval, we can calculate the lower bound using\n",
    "`SDDP.calculate_bound`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nmc = 1000\n",
    "\n",
    "simulations = SDDP.simulate(hydro_model2, Nmc)\n",
    "\n",
    "objective_values = [\n",
    "    sum(stage[:stage_objective] for stage in sim) for sim in simulations\n",
    "]\n",
    "\n",
    "Œº = round(mean(objective_values), digits = 2)\n",
    "\n",
    "ci = round(1.96 * std(objective_values) / sqrt(Nmc), digits = 2)\n",
    "\n",
    "println(\"Confidence interval: \", Œº, \" ¬± \", ci)\n",
    "println(\"Lower bound: \", round(SDDP.calculate_bound(hydro_model2), digits = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Simulating further parameters (Optional)\n",
    "\n",
    "In addition to simulating the primal values of variables, we can also pass\n",
    "`SDDP.jl` custom recorder functions. Each of these functions takes one\n",
    "argument, the JuMP subproblem, and returns anything you can compute. For\n",
    "example, the dual of the demand constraint (which we named\n",
    "`demand_constraint`) corresponds to the price we should charge for\n",
    "electricity, since it represents the cost of each additional unit of demand.\n",
    "To calculate this, we can go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations = SDDP.simulate(\n",
    "    hydro_model2,\n",
    "    1,\n",
    "    custom_recorders = Dict{Symbol, Function}(\n",
    "        :price => (sp) -> JuMP.dual(sp[:demand_constraint])\n",
    "    )\n",
    ")\n",
    "\n",
    "prices = [stage[:price] for stage in simulations[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eea4ee90a24453ed060dbcb6500d3bce",
     "grade": false,
     "grade_id": "cell-254a7258fd2facee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.3 Exercise : Hot water with random demand\n",
    "\n",
    "The actual demand in hot water is not known in advance. We are going to consider that, for each hour, independantly of the others, the actual demand have $30\\%$ chance of being $20\\%$ less than predicted, $40%$ chance of being at prediction level and $30\\%$ chance of being $20\\%$ above prediction level.\n",
    "\n",
    "In addition we are going to introduce a control $v_t$ which correspond to unmet demand, priced at $100$ per unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "71d654edf0d36e44be1453f2e1464308",
     "grade": false,
     "grade_id": "cell-4b86b51c74ca926a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "question"
    ]
   },
   "source": [
    "#### Question 4.\n",
    "\n",
    "Model, train and solve the problem with random demand. Simulate the obtained policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "383fe41241732343e290d07c2bad7432",
     "grade": true,
     "grade_id": "cell-ecfc9963fe7b3c9e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hw_model3 = SDDP.LinearPolicyGraph(\n",
    "    stages = 9, \n",
    "    sense = :Min,\n",
    "    lower_bound = 0.0, \n",
    "    optimizer = with_optimizer(GLPK.Optimizer)\n",
    ") do subproblem, t\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    error(\"to be implemented\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDDP.train(hw_model3; iteration_limit = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nmc = 1000\n",
    "\n",
    "simulations = SDDP.simulate(hw_model3, Nmc)\n",
    "\n",
    "objective_values = [\n",
    "    sum(stage[:stage_objective] for stage in sim) for sim in simulations\n",
    "]\n",
    "\n",
    "Œº = round(mean(objective_values), digits = 2)\n",
    "\n",
    "ci = round(1.96 * std(objective_values) / sqrt(Nmc), digits = 2)\n",
    "\n",
    "println(\"Confidence interval: \", Œº, \" ¬± \", ci)\n",
    "println(\"Lower bound: \", round(SDDP.calculate_bound(hw_model3), digits = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Two uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we created a\n",
    "stochastic hydro-thermal scheduling model. In this tutorial, we extend the\n",
    "problem by adding uncertainty to the fuel costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we assumed that the fuel cost was deterministic: \\\\\\$50/MWh in the\n",
    "first stage, \\\\\\$100/MWh in the second stage, and \\\\\\$150/MWh in the third\n",
    "stage. For this tutorial, we assume that in addition to these base costs, the\n",
    "actual fuel cost is correlated with the inflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new model for the uncertinty is given by the following table:\n",
    "\n",
    "| œâ               |   1 |   2 |    3 |\n",
    "| ----            | --- | --- | ---- |\n",
    "| P(œâ)            | 1/3 | 1/3 |  1/3 |\n",
    "| inflow          |   0 |  50 |  100 |\n",
    "| fuel_multiplier | 1.5 | 1.0 | 0.75 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In stage `t`, the objective is now to minimize:\n",
    "\n",
    "`fuel_multiplier * fuel_cost[t] * thermal_generation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add an uncertain objective, we can simply call `@stageobjective`\n",
    "from inside the `SDDP.parameterize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_model3 = SDDP.LinearPolicyGraph(\n",
    "    stages = 3,\n",
    "    sense = :Min,\n",
    "    lower_bound = 0.0,\n",
    "    optimizer = with_optimizer(GLPK.Optimizer)\n",
    ") do subproblem, t\n",
    "    # Define the state variable.\n",
    "    @variable(subproblem, 0 <= volume <= 200, SDDP.State, initial_value = 200)\n",
    "    # Define the control variables.\n",
    "    @variables(subproblem, begin\n",
    "        thermal_generation >= 0\n",
    "        hydro_generation   >= 0\n",
    "        hydro_spill        >= 0\n",
    "        inflow\n",
    "    end)\n",
    "    # Define the constraints\n",
    "    @constraints(subproblem, begin\n",
    "        volume.out == volume.in + inflow - hydro_generation - hydro_spill\n",
    "        thermal_generation + hydro_generation == 150.0\n",
    "    end)\n",
    "    fuel_cost = [50.0, 100.0, 150.0]\n",
    "    # Parameterize the subproblem.\n",
    "    Œ© = [\n",
    "        (inflow = 0.0, fuel_multiplier = 1.5),\n",
    "        (inflow = 50.0, fuel_multiplier = 1.0),\n",
    "        (inflow = 100.0, fuel_multiplier = 0.75)\n",
    "    ]\n",
    "    SDDP.parameterize(subproblem, Œ©, [1/3, 1/3, 1/3]) do œâ\n",
    "        JuMP.fix(inflow, œâ.inflow)\n",
    "        @stageobjective(subproblem,\n",
    "            œâ.fuel_multiplier * fuel_cost[t] * thermal_generation\n",
    "        )\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training and simulating the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous two tutorials, we train and simulate the policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDDP.train(hydro_model3; iteration_limit = 10)\n",
    "\n",
    "\n",
    "Nmc = 1000\n",
    "simulations = SDDP.simulate(hydro_model3, Nmc,\n",
    "[:volume, :thermal_generation, :hydro_generation, :hydro_spill])\n",
    "\n",
    "objective_values = [\n",
    "    sum(stage[:stage_objective] for stage in sim) for sim in simulations\n",
    "]\n",
    "\n",
    "using Statistics\n",
    "\n",
    "Œº = round(mean(objective_values), digits = 2)\n",
    "ci = round(1.96 * std(objective_values) / sqrt(Nmc), digits = 2)\n",
    "\n",
    "println(\"Confidence interval: \", Œº, \" ¬± \", ci)\n",
    "println(\"Lower bound: \", round(SDDP.calculate_bound(hydro_model3), digits = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous setions, we formulated, solved, and simulated multistage\n",
    "stochastic optimization problems. However, we haven't really investigated what\n",
    "the solution looks like. Luckily, `SDDP.jl` includes a number of plotting\n",
    "tools to help us do that. In this tutorial, we explain the tools and make some\n",
    "pretty pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two plot types help visualize the policy. Thus, we simulate some trajectories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nmc = 1000\n",
    "simulations = SDDP.simulate(hydro_model3, Nmc, \n",
    "    [:volume, :thermal_generation, :hydro_generation, :hydro_spill]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have some data in `simulations` to visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Spaghetti plots (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first plotting utility we discuss is a _spaghetti_ plot (you'll understand\n",
    "the name when you see the graph)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a spaghetti plot, begin by creating a new\n",
    "`SDDP.SpaghettiPlot` instance as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = SDDP.SpaghettiPlot(simulations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add plots to `plt` using the `SDDP.add_spaghetti` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDDP.add_spaghetti(plt; title = \"Reservoir volume\") do data\n",
    "    return data[:volume].out\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't have just return values from the simulation, you can compute things\n",
    "too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDDP.add_spaghetti(\n",
    "    plt; title = \"Fuel cost\", ymin = 0, ymax = 250\n",
    ") do data\n",
    "    if data[:thermal_generation] > 0\n",
    "        return data[:stage_objective] / data[:thermal_generation]\n",
    "    else  # No thermal generation, so return 0.0.\n",
    "        return 0.0\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are many keyword arguments in addition to `title`. For\n",
    "example, we fixed the minimum and maximum values of the y-axis using `ymin`\n",
    "and `ymax`. See the `SDDP.add_spaghetti` documentation for all the\n",
    "arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having built the plot, we now need to display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDDP.save(plt, \"spaghetti_plot.html\", open = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should open a webpage that looks like [this\n",
    "one](../assets/spaghetti_plot.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the mouse, you can highlight individual trajectories by hovering over\n",
    "them. This makes it possible to visualize a single trajectory across multiple\n",
    "dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you click on the plot, then trajectories that are close to the mouse\n",
    "pointer are shown darker and those further away are shown lighter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Publication plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the interactive Javascript plots, you can also create some\n",
    "publication ready plots using the `SDDP.publication_plot` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Info :</b> You need to install the `Plots.jl` \n",
    "    package for this to work. We used the `GR` backend (`gr()`), but any\n",
    "    `Plots.jl` backend should work.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SDDP.publication_plot` implements a plot recipe to create ribbon\n",
    "plots of each variable against the stages. The first argument is the vector of\n",
    "simulation dictionaries and the second argument is the dictionary key that you\n",
    "want to plot. Standard `Plots.jl` keyword arguments such as `title` and `xlabel`\n",
    "can be used to modify the look of each plot. By default, the plot displays\n",
    "ribbons of the 0-100, 10-90, and 25-75 percentiles. The dark, solid line in the\n",
    "middle is the median (i.e. 50'th percentile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "using Plots\n",
    "plot(\n",
    "    SDDP.publication_plot(simulations, title = \"Outgoing volume\") do data\n",
    "        return data[:volume].out\n",
    "    end,\n",
    "    SDDP.publication_plot(simulations, title = \"Thermal generation\") do data\n",
    "        return data[:thermal_generation]\n",
    "    end,\n",
    "    xlabel = \"Stage\",\n",
    "    ylims = (0, 200),\n",
    "    layout = (1, 2),\n",
    "    margin_bottom = 5,\n",
    "    size = (1000, 300)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should open a plot window with a plot that looks like:\n",
    "\n",
    "![publication plot](assets/publication_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save this plot as a PDF using the `Plots.jl` function `savefig`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plots.savefig(\"my_picture.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2765c98395ec5d0200582316332a8361",
     "grade": false,
     "grade_id": "cell-be7dbe835a9c51a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.4 Exercise\n",
    "\n",
    "Plot water stock, heating control and unserved demand for the hot-water tank problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "513efcc2aae8021491dd99f5fd9d1923",
     "grade": true,
     "grade_id": "cell-c30fbeef4770c83b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "error(\"to be implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "099d84d689948e321d9d065cb6edf007",
     "grade": false,
     "grade_id": "cell-e417bccd03b1c0a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5 Exercise : ramp-up constraints (Optional)\n",
    "\n",
    "We are going to consider an example with ramp-up constraints. Consider a thermal production unit.\n",
    "At time she start with an initial stock of $100$. Then at each time step she can produce $u_t$ electricity\n",
    "and sell it for a random price $p_t$.\n",
    "\n",
    "The price is given by a reference price $p_ref$, and we are going to consider that, for each hour, independantly of the others, the price have $30\\%$ chance of being $20\\%$ less than the reference, $40%$ chance of being at reference level and $30\\%$ chance of being $20\\%$ above reference level. \n",
    "\n",
    "\n",
    "The dynamic is simply given by $s_{t+1} = s_t - u_t$. \n",
    "We have bound constraint : $ 0 \\leq s_t \\leq 100$, and $0 \\leq u_t \\leq 25$\n",
    "But we have an additional ramp-up constraint $ |u_t - u_{t-1} | \\leq \\Delta$, with initial control $u_0=0$.\n",
    "\n",
    "Model and solve the problem with `SDDP.jl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38bb8979ba059f33aa506e9bcf57ab56",
     "grade": true,
     "grade_id": "cell-09099c120644ec7e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Œî = 10\n",
    "\n",
    "p_ref = [30 28.5 42.5 50.4 52.6 51.8 48.9 48.6 47.04]\n",
    "\n",
    "\n",
    "th_model = SDDP.LinearPolicyGraph(\n",
    "    stages = 9, \n",
    "    sense = :Min,\n",
    "    lower_bound = -25*sum(p_ref), \n",
    "    optimizer = with_optimizer(GLPK.Optimizer)\n",
    ") do subproblem, t\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    error(\"to be implemented\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08486e46f44f4d789325fdaa30cd02a7",
     "grade": true,
     "grade_id": "cell-a1d7811b5731eb3a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "error(\"to be implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "plot(\n",
    "    SDDP.publication_plot(simulations, title = \"oil stock\") do data\n",
    "        return data[:s].out\n",
    "    end,\n",
    "    SDDP.publication_plot(simulations, title = \"sold electricity\") do data\n",
    "        return data[:u]\n",
    "    end,\n",
    "    SDDP.publication_plot(simulations, title = \"Œ¥\") do data\n",
    "        return data[:u]-data[:u_prev].in\n",
    "    end,\n",
    "    xlabel = \"Stage\",\n",
    "    layout = (1, 3),\n",
    "    margin_bottom = 5,\n",
    "    size = (1000, 300)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
